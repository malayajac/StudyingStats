{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes taken from various online sources, as mentioned below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Primary goal of information theory is to quantify how much information is in data.\n",
    "\n",
    "- Most important metric: entropy.\n",
    "\n",
    "- Defn of entropy for a probability distribution:\n",
    "    - $H = -\\sum_{i=1}^{N} p(x_{i}) \\log{ p(x_{i})}$\n",
    "    \n",
    "    - If we use $\\log_{2}$ in the calculation, we can interpret entropy as the minimum number of bits it would take to encode our information.\n",
    "\n",
    "- But entropy doesn't tell us the optimal encoding scheme to help us achieve this compression.\n",
    "\n",
    "- By simply knowing the theoretical lower bound on the number of bits we need, we have a way to quantify exactly how much information is in our data. Next, we want to quatify how much information is lost when we substitute our observed distribution for a parametrized approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KL divergence is a slight modification of the entropy formula.\n",
    "\n",
    "- Let our probability distribution be $p$, and our approximating probability distribution be $q$.\n",
    "\n",
    "- $$D_{KL}(p||q) = \\sum_{i=1}^{N} p(x_{i}) [\\log{ p(x_{i})} - \\log{ q(x_{i})}] = \\sum_{i=1}^{N} p(x_{i}) \\log{ \\frac{p(x_{i})}{q(x_{i})}}$$\n",
    "    - The KL divergence gives the expectation of the log difference between the prob of data in the original distr with the approximating distr.\n",
    "    \n",
    "    - If we think in terms of $\\log_{2}$, we can interpret this as how many bits of information we expect to lose by using the approximate prob distr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It may be tempting to think of KL divergence as a distance metric, however we cannot use KL divergence to measure the _distance_ between two distributions.\n",
    "    - Why?\n",
    "    \n",
    "    - Because KL divergence is _not symmetric_. This means $D_{KL}(p||q) \\neq D_{KL}(q||p)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/divergence-between-probability-distributions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are many situations where we may want to compare 2 prob distr.\n",
    "\n",
    "- For example, suppose we have a random variable, for which we may have a true distribution and an approximation of that distribution.\n",
    "\n",
    "- Now we want to quantify the difference between these two distributions.\n",
    "\n",
    "- This is done by calculating the statistical distance between the two statistical objects (here, these objects are probability distributions).\n",
    "\n",
    "- But instead, it is more common to calculate a divergence between two probability distributions.\n",
    "    - Divergence is not a symmetrical measure. This means divergence of distr Q from P is different than divergence of distr P from Q."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kullback-Leibler Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When the prob for an event from $P$ is large, but prob for the same event in $Q$ is small, then KL divergence is large.\n",
    "\n",
    "- When the prob for an event from $P$ is small, but prob for the same event in $Q$ is large, then KL divergence is large here also, but not as large as in the first case.\n",
    "\n",
    "- <font color=red>Check this^! How to check this?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When KL=0, that means both distributions are identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KL divergence score is not symmetrical. This means:\n",
    "\n",
    "$$D_{KL}(P|Q) \\neq D_{KL}(Q|P)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Named after Solomon Kullback and Richard Leibler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we are attempting to approximate an unknown probablity distribution, then the target probability distribution from data is P, and Q is our approximation of the distribution.\n",
    "\n",
    "- KL divergence summarizes the number of additional bits/info required to represent an event from the random variable. The better our approximation, the less additional info is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.special import kl_div, rel_entr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define distributions:\n",
    "events = [\"red\", \"green\", \"blue\"]\n",
    "p = [0.10, 0.40, 0.50]\n",
    "q = [0.80, 0.15, 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAFfCAYAAABuqOTCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAGVFJREFUeJzt3X+MXWd95/H3B7uuRRJArQdRxR7GAvPDAkS6E6coBcISqLMp9kKyi71qRXZZLNQ1YTfNas2PRitnVaVBbFSppouhEQFETEhRGRIXwwIBwRJkJ0AW23gZmYCnQcT8UGhoITV89485htvJnWQmGZ95PH6/pNGc5znPPfO15+h+5vy4z0lVIUmS2vWExS5AkiQ9MsNakqTGGdaSJDXOsJYkqXGGtSRJjTOsJUlqnGEtSVLjDGtJkhpnWEuS1Ljli13AQli1alWNjY0tdhmSJM3LXXfd9f2qGnm0cUsirMfGxjhw4MBilyFJ0rwk+fZcxnkaXJKkxhnWkiQ1rvewTrIxyZEkk0l2DFl/RZLjSb7aff3HvmuUJKklvV6zTrIM2AW8ApgC9ieZqKpDM4Z+uKq291mbJEmt6vvIegMwWVVHq+ohYA+wuecaJEk6rfR9N/i5wLGB9hRwwZBxlyV5CfD/gP9SVcdmDkiyDdgGMDo6egpKlaSFN7bj9sUuQY/Dvddduig/t+8j6wzpqxntjwNjVfUC4H8DNw3bUFXtrqrxqhofGXnUj6hJknTa6jusp4A1A+3VwH2DA6rqB1X1s675HuBf9FSbJElN6jus9wPrkqxNsgLYAkwMDkjyWwPNTcDhHuuTJKk5vV6zrqoTSbYD+4BlwI1VdTDJTuBAVU0AVybZBJwAfghc0WeNkiS1pvfpRqtqL7B3Rt81A8tvAd7Sd12SJLXKGcwkSWqcYS1JUuMMa0mSGmdYS5LUOMNakqTGGdaSJDXOsJYkqXGGtSRJjTOsJUlqnGEtSVLjDGtJkhpnWEuS1DjDWpKkxhnWkiQ1zrCWJKlxhrUkSY0zrCVJapxhLUlS43oP6yQbkxxJMplkxyOMuzxJJRnvsz5JklrTa1gnWQbsAi4B1gNbk6wfMu4c4Ergy33WJ0lSi/o+st4ATFbV0ap6CNgDbB4y7lrgeuCnfRYnSVKLlvf8884Fjg20p4ALBgckOQ9YU1W3Jbl6tg0l2QZsAxgdHT0FpUpzM7bj9sUuQY/DvdddutglSI+q7yPrDOmrX65MngDcAPzxo22oqnZX1XhVjY+MjCxgiZIktaXvsJ4C1gy0VwP3DbTPAZ4H3JHkXuB3gAlvMpMkncn6Duv9wLoka5OsALYAEydXVtUDVbWqqsaqagy4E9hUVQd6rlOSpGb0GtZVdQLYDuwDDgO3VNXBJDuTbOqzFkmSThd932BGVe0F9s7ou2aWsRf1UZMkSS1zBjNJkhpnWEuS1DjDWpKkxhnWkiQ1zrCWJKlxhrUkSY0zrCVJapxhLUlS4wxrSZIaZ1hLktQ4w1qSpMYZ1pIkNc6wliSpcYa1JEmNM6wlSWqcYS1JUuMMa0mSGmdYS5LUuN7DOsnGJEeSTCbZMWT9G5P83yRfTfKFJOv7rlGSpJb0GtZJlgG7gEuA9cDWIWH8oap6flW9ELge+J991ihJUmv6PrLeAExW1dGqegjYA2weHFBVPx5ongVUj/VJktSc5T3/vHOBYwPtKeCCmYOS/CfgKmAF8C/7KU2SpDb1fWSdIX0PO3Kuql1V9QzgvwFvH7qhZFuSA0kOHD9+fIHLlCSpHX2H9RSwZqC9GrjvEcbvAf71sBVVtbuqxqtqfGRkZAFLlCSpLX2H9X5gXZK1SVYAW4CJwQFJ1g00LwW+2WN9kiQ1p9dr1lV1Isl2YB+wDLixqg4m2QkcqKoJYHuSi4F/An4EvK7PGiVJak3fN5hRVXuBvTP6rhlYfnPfNUmS1DJnMJMkqXGGtSRJjTOsJUlqnGEtSVLj5n2DWfeRqyuYnjr0t4DvAl8GbuqmEJUkSQtoXkfWSZ7L9OeedwHPA37efd8FTPqELEmSFt58j6x3Aw8AL66q75zsTDIK3A78L+AlC1eeJEma7zXrceCawaAG6NrXAOcvVGGSJGnafMP6XmDlLOtWAt+ZZZ0kSXqM5hvWO4D/keSfPdYyye8AO5l+SpYkSVpA871m/XbgScD/SXI/cD/w1O7rB8Bbk7z15OCq2rBQhUqSdKaab1h/vfuSJEk9mVdYV9W/P1WFSJKk4ZzBTJKkxhnWkiQ1zrCWJKlxhrUkSY0zrCVJapxhLUlS43oP6yQbkxxJMplkx5D1VyU5lOSeJJ9O8vS+a5QkqSW9hnWSZUw/TvMSYD2wdchjNb8CjFfVC4Bbgev7rFGSpNb0fWS9AZisqqNV9RCwB9g8OKCqPltV/9A17wRW91yjJElN6TuszwWODbSnur7ZvB7422ErkmxLciDJgePHjy9giZIktaXvsM6Qvho6MPkDpp+f/Y5h66tqd1WNV9X4yMjIApYoSVJb5vsgj8drClgz0F4N3DdzUJKLgbcBL62qn/VUmyRJTer7yHo/sC7J2iQrgC3AxOCAJOcB7wY2VdX9PdcnSVJzeg3rqjoBbAf2AYeBW6rqYJKdSTZ1w94BnA18JMlXk0zMsjlJks4IfZ8Gp6r2Antn9F0zsHxx3zVJktQyZzCTJKlxhrUkSY0zrCVJapxhLUlS4wxrSZIaZ1hLktQ4w1qSpMYZ1pIkNc6wliSpcYa1JEmNM6wlSWqcYS1JUuMMa0mSGmdYS5LUOMNakqTGGdaSJDXOsJYkqXGGtSRJjes9rJNsTHIkyWSSHUPWvyTJ3UlOJLm87/okSWpNr2GdZBmwC7gEWA9sTbJ+xrDvAFcAH+qzNkmSWrW855+3AZisqqMASfYAm4FDJwdU1b3dul/0XJskSU3q+zT4ucCxgfZU1ydJkmbR95F1hvTVY9pQsg3YBjA6Ovp4anqYsR23L+j21K97r7t0sUuQpAXV95H1FLBmoL0auO+xbKiqdlfVeFWNj4yMLEhxkiS1qO+w3g+sS7I2yQpgCzDRcw2SJJ1Weg3rqjoBbAf2AYeBW6rqYJKdSTYBJDk/yRTwb4B3JznYZ42SJLWm72vWVNVeYO+MvmsGlvczfXpckiThDGaSJDXPsJYkqXGGtSRJjTOsJUlqnGEtSVLjDGtJkhpnWEuS1DjDWpKkxhnWkiQ1zrCWJKlxhrUkSY0zrCVJapxhLUlS4wxrSZIaZ1hLktQ4w1qSpMYZ1pIkNc6wliSpcYa1JEmN6z2sk2xMciTJZJIdQ9b/epIPd+u/nGSs7xolSWpJr2GdZBmwC7gEWA9sTbJ+xrDXAz+qqmcCNwB/1meNkiS1pu8j6w3AZFUdraqHgD3A5hljNgM3dcu3Ai9Pkh5rlCSpKct7/nnnAscG2lPABbONqaoTSR4AfhP4/uCgJNuAbV3zwSRHTknFS9MqZvx/LiXxXMxCc3/RfLi/zM/T5zKo77AedoRcj2EMVbUb2L0QRZ1pkhyoqvHFrkOnB/cXzYf7y6nR92nwKWDNQHs1cN9sY5IsB54M/LCX6iRJalDfYb0fWJdkbZIVwBZgYsaYCeB13fLlwGeq6mFH1pIknSl6PQ3eXYPeDuwDlgE3VtXBJDuBA1U1AfwV8IEkk0wfUW/ps8YzhJcPNB/uL5oP95dTIB60SpLUNmcwkySpcYa1JEmNM6wFQJKLkty22HVIalOSsSRfH9J/RxI/qnWKGdZLXKb5e9acdB+XlNQY38SXoO4v4MNJ3gXcDfxhki8luTvJR5Kc3Y3bmOQbSb4AvGZRi1YvkvxJ9zv/VJKbk1zdHRn9aZLPAW9OMpLkr5Ps774u7F57VpIbu76vJNnc9V+R5KNJPpHkm0muX9R/pE6l5UluSnJPkluTPHFwZZIHB5YvT/K+bnnoPqW5M6yXrmcD7wdewfTDUS6uqt8GDgBXJVkJvAd4FfBi4GmLVaj60Z2qvAw4j+k/zgZPXT6lql5aVe8E/hy4oarO78a/txvzNqbnPTgfeBnwjiRndeteCLwWeD7w2iSDkx9p6Xg2sLuqXgD8GPijOb5utn1Kc+Qpr6Xr21V1Z5LfZ/oJZ1/snoeyAvgS8BzgW1X1TYAkH+RXc61rafpd4GNV9Y8AST4+sO7DA8sXA+sHnp/zpCTnAK8ENiW5uutfCYx2y5+uqge67R5ier7jwecAaGk4VlVf7JY/CFw5x9cN3aeq6u8XusClyrBeun7SfQ/wqaraOrgyyQsZMue6lrRHenrdTwaWnwC86GSo//LF0++0l1XVkRn9FwA/G+j6Ob63LFUz3zMeqb1yYHnoPqW58zT40ncncGGSZwIkeWKSZwHfANYmeUY3butsG9CS8QXgVUlWdvctXDrLuE8C2082uj/sYHrmwTedfGRtkvNOZbFq0miSF3XLW5nepwZ9L8lzu5taXz3QP9s+pTkyrJe4qjoOXAHcnOQepsP7OVX1U6ZPe9/e3WD27cWrUn2oqv1Mz73/NeCjTN+/8MCQoVcC491NRIeAN3b91wK/BtzTfYTn2lNftRpzGHhd917yG8Bfzli/A7gN+Azw3YH+2fYpzZHTjUpnkCRnV9WD3V28nwe2VdXdi12XpEfmdSXpzLI7yXqmryfeZFBLpwePrCVJapzXrCVJapxhLUlS4wxrSZIaZ1hLktQ4w1qSpMYZ1pIkNc6wliSpcYa1JEmNM6wlSWqcYS1JUuOWxNzgq1atqrGxscUuQ5Kkebnrrru+X1UjjzZuSYT12NgYBw4cWOwyJEmalyRzejyxp8ElSWpc72GdZGOSI0kmk+wYsn40yWeTfKV7UPm/6rtGSZJa0mtYJ1kG7AIuAdYDW7tn6w56O3BLVZ0HbAHe1WeNkiS1pu8j6w3AZFUdraqHgD3A5hljCnhSt/xk4L4e65MkqTl932B2LnBsoD0FXDBjzH8HPpnkTcBZwMX9lCZJUpv6DusM6asZ7a3A+6rqnUleBHwgyfOq6hf/bEPJNmAbwOjo6IIWObbj9gXdnvp173WXLnYJkrSg+j4NPgWsGWiv5uGnuV8P3AJQVV8CVgKrZm6oqnZX1XhVjY+MPOpH1CRJOm31Hdb7gXVJ1iZZwfQNZBMzxnwHeDlAkucyHdbHe61SkqSG9BrWVXUC2A7sAw4zfdf3wSQ7k2zqhv0x8IYkXwNuBq6oqpmnyiVJOmP0PoNZVe0F9s7ou2Zg+RBwYd91SZLUKmcwkySpcYa1JEmNM6wlSWqcYS1JUuMMa0mSGmdYS5LUOMNakqTGGdaSJDXOsJYkqXGGtSRJjTOsJUlqnGEtSVLjDGtJkhpnWEuS1DjDWpKkxhnWkiQ1zrCWJKlxhrUkSY0zrCVJapxhLUlS4wxrSZIaZ1hLktQ4w1qSpMYZ1pIkNc6wliSpcYa1JEmN6z2sk2xMciTJZJIds4z5t0kOJTmY5EN91yhJUkuW9/nDkiwDdgGvAKaA/UkmqurQwJh1wFuAC6vqR0me2meNkiS1pu8j6w3AZFUdraqHgD3A5hlj3gDsqqofAVTV/T3XKElSU/oO63OBYwPtqa5v0LOAZyX5YpI7k2wctqEk25IcSHLg+PHjp6hcSZIWX99hnSF9NaO9HFgHXARsBd6b5CkPe1HV7qoar6rxkZGRBS9UkqRW9B3WU8CagfZq4L4hYz5WVf9UVd8CjjAd3pIknZH6Duv9wLoka5OsALYAEzPG/A3wMoAkq5g+LX601yolSWpIr2FdVSeA7cA+4DBwS1UdTLIzyaZu2D7gB0kOAZ8F/mtV/aDPOiVJakmvH90CqKq9wN4ZfdcMLBdwVfclSdIZzxnMJElqnGEtSVLjDGtJkhpnWEuS1DjDWpKkxhnWkiQ1zrCWJKlxhrUkSY0zrCVJapxhLUlS4wxrSZIaZ1hLktQ4w1qSpMYZ1pIkNc6wliSpcYa1JEmNM6wlSWqcYS1JUuMMa0mSGmdYS5LUOMNakqTGGdaSJDVu+VwHJnnJfDZcVZ+ffzmSJGmmOYc1cAdQA+0MaZ9UwLLHXpYkSTppPmH9SuBG4BPAR4H7gacClwG/B/wH4LsLXaAkSWe6+VyzfhPw/qraVlWfqKq7u+9vAN4P/OeqOnjya7aNJNmY5EiSySQ7HmHc5Ukqyfg8apQkacmZT1i/HPjcLOs+B1z0aBtIsgzYBVwCrAe2Jlk/ZNw5wJXAl+dRnyRJS9J8wvqHwOZZ1r26W/9oNgCTVXW0qh4C9syyzWuB64GfzqM+SZKWpPlcs74O+IskY8AEv7pmvZnpI+Xtc9jGucCxgfYUcMHggCTnAWuq6rYkV8+2oSTbgG0Ao6Ojc/5HSJJ0uplzWFfVu5L8HfBW4C+6154Avgq8pqr+Zg6byZC+X95RnuQJwA3AFXOoZzewG2B8fLweZbgkSaet+RxZU1UfAz7WheoIcLyqfjGPTUwBawbaq4H7BtrnAM8D7kgC8DRgIsmmqjown1olSVoq5hXWJ3UB/b3H8NL9wLoka4G/A7YA/25guw8Aq062k9wBXG1QS5LOZL1ON1pVJ5i+tr0POAzcUlUHk+xMsqnPWiRJOl08piPrx6Oq9gJ7Z/RdM8vYi/qoSZKklvkgD0mSGmdYS5LUOMNakqTGGdaSJDXOsJYkqXGGtSRJjTOsJUlqnGEtSVLjDGtJkhpnWEuS1DjDWpKkxhnWkiQ1zrCWJKlxhrUkSY0zrCVJapxhLUlS4wxrSZIaZ1hLktQ4w1qSpMYZ1pIkNc6wliSpcYa1JEmNM6wlSWqcYS1JUuMMa0mSGtd7WCfZmORIkskkO4asvyrJoST3JPl0kqf3XaMkSS3pNayTLAN2AZcA64GtSdbPGPYVYLyqXgDcClzfZ42SJLWm7yPrDcBkVR2tqoeAPcDmwQFV9dmq+oeueSewuucaJUlqSt9hfS5wbKA91fXN5vXA357SiiRJatzynn9ehvTV0IHJHwDjwEtnWb8N2AYwOjq6UPVJktScvo+sp4A1A+3VwH0zByW5GHgbsKmqfjZsQ1W1u6rGq2p8ZGTklBQrSVIL+g7r/cC6JGuTrAC2ABODA5KcB7yb6aC+v+f6JElqTq9hXVUngO3APuAwcEtVHUyyM8mmbtg7gLOBjyT5apKJWTYnSdIZoe9r1lTVXmDvjL5rBpYv7rsmSZJa5gxmkiQ1zrCWJKlxhrUkSY3r/Zq1tNSM7bh9sUvQ43DvdZcudgnSo/LIWpKkxhnWkiQ1zrCWJKlxhrUkSY0zrCVJapxhLUlS4wxrSZIaZ1hLktQ4w1qSpMYZ1pIkNc6wliSpcYa1JEmNM6wlSWqcYS1JUuMMa0mSGmdYS5LUuOWLXYAknUnGdty+2CXocbj3uksX5ed6ZC1JUuMMa0mSGmdYS5LUOMNakqTGGdaSJDWu97BOsjHJkSSTSXYMWf/rST7crf9ykrG+a5QkqSW9hnWSZcAu4BJgPbA1yfoZw14P/KiqngncAPxZnzVKktSavo+sNwCTVXW0qh4C9gCbZ4zZDNzULd8KvDxJeqxRkqSm9D0pyrnAsYH2FHDBbGOq6kSSB4DfBL4/OCjJNmBb13wwyZFTUvHStIoZ/59LSTwXs9DcXzQf7i/z8/S5DOo7rIcdIddjGENV7QZ2L0RRZ5okB6pqfLHr0OnB/UXz4f5yavR9GnwKWDPQXg3cN9uYJMuBJwM/7KU6SZIa1HdY7wfWJVmbZAWwBZiYMWYCeF23fDnwmap62JG1JElnil5Pg3fXoLcD+4BlwI1VdTDJTuBAVU0AfwV8IMkk00fUW/qs8Qzh5QPNh/uL5sP95RSIB62SJLXNGcwkSWqcYS0AklyU5LbFrkNSm5KMJfn6kP47knj39ylmWC9xmebvWXPSfQJDUmN8E1+Cur+ADyd5F3A38IdJvpTk7iQfSXJ2N25jkm8k+QLwmkUtWr1I8ifd7/xTSW5OcnV3ZPSnST4HvDnJSJK/TrK/+7qwe+1ZSW7s+r6SZHPXf0WSjyb5RJJvJrl+Uf+ROpWWJ7kpyT1Jbk3yxMGVSR4cWL48yfu65aH7lObOsF66ng28H3gF0/OtX1xVvw0cAK5KshJ4D/Aq4MXA0xarUPWjO1V5GXAe03+cDZ66fEpVvbSq3gn8OXBDVZ3fjX9vN+ZtTH+U8nzgZcA7kpzVrXsh8Frg+cBrkwzOp6Cl49nA7qp6AfBj4I/m+LrZ9inNkae8lq5vV9WdSX6f6YemfLGbYn0F8CXgOcC3quqbAEk+yK+mb9XS9LvAx6rqHwGSfHxg3YcHli8G1g9Myf+kJOcArwQ2Jbm6618JjHbLn66qB7rtHmJ6CsXBqYW1NByrqi92yx8Erpzj64buU1X19wtd4FJlWC9dP+m+B/hUVW0dXJnkhQyZxlVL2iM9EOcnA8tPAF50MtR/+eLpd9rLqurIjP4LgJ8NdP0c31uWqpnvGY/UXjmwPHSf0tx5GnzpuxO4MMkzAZI8McmzgG8Aa5M8oxu3dbYNaMn4AvCqJCu7+xYunWXcJ4HtJxvdH3YwPZnRm04+BS/JeaeyWDVpNMmLuuWtTO9Tg76X5LndTa2vHuifbZ/SHBnWS1xVHQeuAG5Ocg/T4f2cqvop06e9b+9uMPv24lWpPlTVfqan8/0a8FGm7194YMjQK4Hx7iaiQ8Abu/5rgV8D7uk+wnPtqa9ajTkMvK57L/kN4C9nrN8B3AZ8BvjuQP9s+5TmyBnMpDNIkrOr6sHuLt7PA9uq6u7FrkvSI/O6knRm2Z1kPdPXE28yqKXTg0fWkiQ1zmvWkiQ1zrCWJKlxhrUkSY0zrCVJapxhLUlS4wxrSZIa9/8B4ngHz/dpG1UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_axes([0, 0, 1, 0.5])\n",
    "ax2 = fig.add_axes([0, -0.6, 1, 0.5])\n",
    "\n",
    "ax1.bar(events, p)\n",
    "ax2.bar(events, q)\n",
    "\n",
    "ax1.set_ylabel(\"p\", fontsize=15)\n",
    "ax2.set_ylabel(\"q\", fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_divergence_bits(p, q):\n",
    "    MySum = 0.0\n",
    "    for ii in range(len(p)):\n",
    "        MySum += p[ii] * np.log2(p[ii]/q[ii])\n",
    "    return MySum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_divergence_nats(p, q):\n",
    "    MySum = 0.0\n",
    "    for ii in range(len(p)):\n",
    "        MySum += p[ii] * np.log(p[ii]/q[ii])\n",
    "    return MySum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL(P||Q) = 1.9269790471552186 bits\n",
      "KL(Q||P) = 2.0216479703638055 bits\n"
     ]
    }
   ],
   "source": [
    "# KL(P||Q):\n",
    "print(\"KL(P||Q) =\", KL_divergence_bits(p, q), \"bits\")\n",
    "\n",
    "# KL(Q||P):\n",
    "print(\"KL(Q||P) =\", KL_divergence_bits(q, p), \"bits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL(P||Q) = 1.3356800935337299 nats\n",
      "KL(Q||P) = 1.4012995907424075 nats\n"
     ]
    }
   ],
   "source": [
    "# KL(P||Q):\n",
    "print(\"KL(P||Q) =\", KL_divergence_nats(p, q), \"nats\")\n",
    "\n",
    "# KL(Q||P):\n",
    "print(\"KL(Q||P) =\", KL_divergence_nats(q, p), \"nats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scipy has in-built functions ```kl_div()``` and ```rel_entr()```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\textrm{kl_div}(p, q)\n",
    "=\n",
    "\\begin{cases} \n",
    "    p \\log(p/q) - p + q & \\text{if } p>0, q>0 \\\\\n",
    "    q & \\text{if } p = 0, q \\geq 0 \\\\\n",
    "    \\infty & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\textrm{rel_entr}(p, q)\n",
    "=\n",
    "\\begin{cases} \n",
    "    p \\log(p/q) & \\text{if } p>0, q>0 \\\\\n",
    "    0 & \\text{if } p = 0, q \\geq 0 \\\\\n",
    "    \\infty & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.33568009353373\n",
      "1.4012995907424075\n"
     ]
    }
   ],
   "source": [
    "print( sum(kl_div(p, q)) )\n",
    "print( sum(kl_div(q, p)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3356800935337299\n",
      "1.4012995907424075\n"
     ]
    }
   ],
   "source": [
    "print( sum(rel_entr(p, q)) )\n",
    "print( sum(rel_entr(q, p)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we have to sum over the outputs of ```kl_div()``` and ```rel_entr()``` in order to get a scalar quantity. This is because of the way these functions have been defined in scipy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that both ```kl_div()``` and ```rel_entr()``` use natural log in their calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that relative \"relative entropy\" is often used as a synonym for \"KL divergence\". (See https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n",
    "\n",
    "But, clearly, that is not the case in scipy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jensen-Shannon Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- JSD is another way to quantify the difference or similarity between 2 probability distributions.\n",
    "\n",
    "\n",
    "- It uses the KL divergence to calculate a normalized score that is symmetrical. This means that the divergence of P from Q is the same as divergence of Q from P:<br>\n",
    "$\\textrm{JSD}(P||Q) = \\textrm{JSD}(Q||P)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defn:\n",
    "$$\\textrm{JSD}(P||Q) = \\frac{1}{2} D_{KL}(P||M) + \\frac{1}{2} D_{KL}(Q||M)$$\n",
    "\n",
    "where,\n",
    "$$M = \\frac{1}{2} (P + Q)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- JSD provides a smoothed and normalized version of KL divergence.\n",
    "\n",
    "- JSD $\\in [0, \\ln{2}]$ (when using natural log in calculation).\n",
    "\n",
    "- When the two probability distributions are identical, then JSD $= 0$.\n",
    "\n",
    "- When the two probability distributions are maximally different, then JSD $= \\ln{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Defn: $\\sqrt{\\textrm{JSD}}$ = Jensen-Shannon distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JSD_bits(P, Q):\n",
    "    #assume P and Q are lists.\n",
    "    M = 0.5 * (np.array(P) + np.array(Q))\n",
    "    jsd_score = 0.5*KL_divergence_bits(P, M) + 0.5*KL_divergence_bits(Q, M)\n",
    "    return jsd_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JSD_nats(P, Q):\n",
    "    #assume P and Q are lists.\n",
    "    M = 0.5 * (np.array(P) + np.array(Q))\n",
    "    jsd_score = 0.5*sum(rel_entr(P, M)) + 0.5*sum(rel_entr(Q, M))\n",
    "    return jsd_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JS divergence:\n",
      "JSD(P||Q) = 0.42020057037637326 bits\n",
      "JSD(Q||P) = 0.42020057037637326 bits\n",
      "or,\n",
      "JSD(P||Q) = 0.29126084062606405 nats\n",
      "JSD(Q||P) = 0.29126084062606405 nats\n"
     ]
    }
   ],
   "source": [
    "print(\"JS divergence:\")\n",
    "print(\"JSD(P||Q) = {} bits\".format(JSD_bits(p, q)))\n",
    "print(\"JSD(Q||P) = {} bits\".format(JSD_bits(p, q)))\n",
    "print(\"or,\")\n",
    "print(\"JSD(P||Q) = {} nats\".format(JSD_nats(p, q)))\n",
    "print(\"JSD(Q||P) = {} nats\".format(JSD_nats(p, q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JS distance:\n",
      "JS(P||Q) distance = 0.6482287947757128 bits\n",
      "JS(Q||P) distance = 0.6482287947757128 bits\n",
      "or,\n",
      "JS(P||Q) distance = 0.5396858721757167 nats\n",
      "JS(Q||P) distance = 0.5396858721757167 nats\n"
     ]
    }
   ],
   "source": [
    "print(\"JS distance:\")\n",
    "print(\"JS(P||Q) distance = {} bits\".format(np.sqrt(JSD_bits(p, q))) )\n",
    "print(\"JS(Q||P) distance = {} bits\".format(np.sqrt(JSD_bits(p, q))) )\n",
    "print(\"or,\")\n",
    "print(\"JS(P||Q) distance = {} nats\".format(np.sqrt(JSD_nats(p, q))) )\n",
    "print(\"JS(Q||P) distance = {} nats\".format(np.sqrt(JSD_nats(p, q))) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JS distance is also given in ```scipy.spatial.distance.jensenshannon```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import jensenshannon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JS distance = 0.6482287947757129 bits\n",
      "JS distance = 0.5396858721757167 nats\n"
     ]
    }
   ],
   "source": [
    "print( \"JS distance = {} bits\".format(jensenshannon(p, q, 2)) )\n",
    "print( \"JS distance = {} nats\".format(jensenshannon(p, q)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
